%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************

\chapter{Design and Analysis}

\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi


\section{Partitioning Scheme}

We implement the idea of partitioning from \cite{DBLP} onto the gMatrix.

A data sample is needed to perform the partitioning. In the experiment, we perform reservoir sampling on the original dataset to obtain the data sample. The size of the data sample is 5\% of the original size of the dataset.

The gMatrix is divided into partitions. Each partition is associated with a set of source vertices. Each source vertex is then only associated with exactly one partition. Edge $(u,v)$ is stored in one of the partitions if and only if $u$ is in the set of source vertices associated with the partition. The idea of the partitioning is to group edges with similar frequencies in the same partition to improve sketch accuracy.

The statistical variances of the frequencies of edges (from the data sample) with common source vertex are computed. We select source vertices which have variances not exceeding a certain threshold (variance of 100 is used in the experiment). An outlier partition is reserved to store frequencies of edges in the dataset whose source vertex is either not found in the data sample or are not suitable for the purpose of the partitioning, i.e. the statistical variance of the frequencies of all edges with the same source vertex in the data sample is exceeding the threshold.

Determining the right outlier partition size is very critical for the effectiveness of the partitioning, even more so than in partitioning the gSketch\cite{DBLP}. We determine the outlier partition size ratio by estimating the outlier ratio from the data sample. We do so by first splitting the data sample into two; the first consists of 90\% of the whole data sample, and the other split consists of the rest; then, after selecting the suitable source vertices from the first split, the ratio of outliers in the second split is computed, which is essentially the ratio of the number of source vertices in the split not found among the previously-selected source vertices, to the size of the split. Then, it is assigned as the outlier partition size ratio.

The partitioning algorithm is no different than in \cite{DBLP}. 

\begin{algorithm}
\caption{Sketch-Partitioning (Data Sample: $D$)}\label{euclid}
\begin{algorithmic}[1]

\State Create a root node $S$ of the partitioning tree as an
active node;
\State $S.sides \gets h$;
\State $S.depth \gets d$;
\State Create an empty list $L$ containing $S$ only;
\While{$L \neq \varnothing$ }
\State Partition active node $S \in L$ based on D into $S_1$, $S_2$ by minimizing overall relative error;
\State $S_1.rows = S_2.rows = \frac{S.rows}{2};$
\State $L \gets L \setminus \left \{ S \right \};$
\For{$i \in \left[ 1..2 \right]$}
    \If{$(S_i.sides >= w_0)$ and ($\sum_{v \in V_S} \tilde{d}(v) \leqslant C \cdot S_i.sides$)}
    
    \State $L \gets L \cup S_i$;
    \Else
    \State Construct the localized sketch $S_i$;
    
    \EndIf
\EndFor
\EndWhile

\end{algorithmic}
\end{algorithm}

The suitable source vertices are sorted based on non-decreasing average frequency of edges emanating from them $f_v/d_v$ in the data sample, where $f_v$ is the sum of frequencies of all edges in the data sample with source vertex $v$, and $d_v$ is the out-degree of vertex $v$. The sorted source vertex set is then recursively split into two, and each split minimizes the overall relative error E. \[E = \sum_{v \in S_1} \frac{d_v \cdot F_{S_1}}{f_v / d_v} + \sum_{v \in S_2} \frac{d_v \cdot F_{S_1}}{f_v / d_v}\]

The recursion stops when either the size of the partition becomes small enough, i.e. the number of rows is less than a user-specified threshold $r_0$ (which is fixed at 100 in this experiment), or when the probability of collision in any particular cell in the sketch can be bounded above by a user-speficied threshold $C$, which is the case when the density of distinct edges, $\sum_{v \in s} d_v/(S.rows \cdot S.cols)$, is also bounded above by $C$, as proven in \cite{DBLP}.


\section{Edge Frequency Estimation}

\subsection{Description}
Since for each source vertex, there is only one partition that is responsible for storing any edges incident on it, to find the estimated frequency for an arbitrary edge $(i,j)$:

\begin{enumerate}
  \item Find partition $P$ that is responsible for $i$
  \item Find the minimum of $V_P(g_k(i),g_k(j),k)$ for all $k \in \{1..w\}$, where $V_P$ is the value of the cell in the partition $P$. 
\end{enumerate}

\subsection{Analysis}

Let $S$ be a gMatrix of size $h\times h\times w$. Let $U$ be the set of source vertices in the data sample. Suppose the partitioning step is performed already. Note that any of the produced partitions will have number of rows equal to $\frac{h}{2^d}$ where $d$ is the depth of the recursion in which the partition is built. Let P be an arbitrary partition. The size of P is $\frac{h}{2^d}\times h\times w$. Let $U_P$ be the set of source vertices in the data sample associated with $P$.



\begin{theorem}
\label{probguarantee}
  Let $(i,j)$ be an edge in $P$, $Q(i,j)$ be its actual frequency, and $\overline{Q(i,j)}$ be its estimated frequency according to partition P. Let $L_P$ be the total frequency of edges received so far in the arbitrary partition, i.e. total frequency of edges $(u,v)$ such that $u \in U_P$. Let $A_P(j)$ be the sum of the frequencies of edges $(u,v)$ on $P$ such that $v=j$. Let $B_P(i)$ be the sum of the frequencies of edges $(u,v)$ on $P$ such that $u=i$. Then,
  
  \[
P(Q(i,j) \leq \overline{Q(i,j)} \leq Q(i,j) + L_P \cdot \epsilon + A_P(j) \cdot h \cdot \epsilon + B_P(i) \cdot h \cdot \epsilon) \geq 1-(\frac{2^{d+1}+1}{h^2\cdot\epsilon})^w
\]

\end{theorem}

\begin{proof}
  Note that $\overline{Q(i,j)}$ is essentially $Q(i,j)$ added with the frequencies of spurious edges mapped into the same cell $(g_k(i),g_k(j),k)$ in the partition, so $P(Q(i,j) \leq \overline{Q(i,j)}) = 1$. We remain to show that

\[
P(\overline{Q(i,j)} \leq Q(i,j) + L_P \cdot \epsilon + A_P(j) \cdot h \cdot \epsilon + B_P(i) \cdot h \cdot \epsilon) \geq 1-(\frac{2^{d+1}+1}{h^2\cdot\epsilon})^w
\]

  There are three possible cases for the spurious edges.
  
The first is those spurious edges $(u,v)$ for which $u \neq i$ and $v \neq j$. Any of such edges are equally likely to be mapped into any cell in $P$, and the probability to be mapped into any particular cell is $\frac{1}{h \cdot \frac{h}{2^d}} = \frac{2^d}{h^2}$. Let $X_k$ be a random variable that represents the number of such spurious edges that are mapped into $(g_k(i),g_k(j),k)$. Therefore, $E[X_k] = \frac{2^dL_P}{h^2}$. Then, by Markov's inequality,

\begin{equation} \label{efreq1}
P(X_k > L_P \cdot \epsilon) \leq \frac{E[X_k]}{L_P \cdot \epsilon} = \frac{2^d}{h^2\epsilon}
\end{equation}

The second case of spurious edges $(u,v)$ are those for which $u \neq i$ but $v=j$. The probability that any of such edges to be mapped into $(g_k(i),g_k(j),k)$ in $P$ is $\frac{1}{\frac{h}{2^d}} = \frac{2^d}{h}$. Let $Y_k$ be a random variable representing the number of such spurious edges. Therefore, $E[Y_k] = \frac{2^dA_P(j)}{h}$. By Markov's inequality,

\begin{equation} \label{efreq2}
P(Y_k > h A_P(j) \cdot \epsilon) \leq \frac{E[Y_k]}{h A_P(j) \cdot \epsilon} = \frac{2^d}{h^2\epsilon}
\end{equation}

The third case of spurious edges $(u,v)$ are those for which $u=i$ but $v \neq j$. The probability that any of such edges to be mapped into $(g_k(i),g_k(j),k)$ in $P$ is $\frac{1}{h}$. Let $Z_k$ be a random variable representing the number of such spurious edges. Therefore, $E[Z_k] = \frac{B_P(i)}{h}$. By Markov's inequality,

\begin{equation} \label{efreq3}
P(Z_k > h B_P(i) \cdot \epsilon) \leq \frac{E[Z_k]}{h B_P(i) \cdot \epsilon} = \frac{1}{h^2\epsilon}
\end{equation}

Combining equations \ref{efreq1}, \ref{efreq2}, \ref{efreq3},
\begin{equation} \label{efreq4}
  P(X_k + Y_k + Z_k > L_P \cdot \epsilon + A_P(j) \cdot h \cdot \epsilon + B_P(i) \cdot h \cdot \epsilon) \leq \frac{2^{d+1}+1}{h^2\cdot\epsilon}
\end{equation}

Therefore,
\begin{align}
\begin{split}
&  P(\overline{Q(i,j)} \leq Q(i,j) + L_P \cdot \epsilon + A_P(i) \cdot h \cdot \epsilon + B_P(i) \cdot h \cdot \epsilon)
\\  &= 1 - \prod _{k=1}^{w}P(X_k + Y_k + Z_k > L_P \cdot \epsilon + A_P(j) \cdot h \cdot \epsilon + B_P(i) \cdot h \cdot \epsilon)
\\  &\geq 1-(\frac{2^{d+1}+1}{h^2\cdot\epsilon})^w
\end{split}
\end{align}

\end{proof}

\begin{remarks}
In Theorem \ref{probguarantee}, the probability of the guarantee gets lower as the depth $d$ of the recursion in which a partition is built gets deeper, but in ideal partitioning, the guarantee of the estimate of the partition itself gets better as $L_p$, $A_P(i)$, and $B_P(i)$ are reduced.
\end{remarks}


\section{Heavy-hitter Edge Queries}
\subsection{Description}
\subsection{Analysis}
